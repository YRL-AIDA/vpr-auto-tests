{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41377257-07a1-4b38-9178-f0e54c4fb8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('..') # append parent directory, we need it\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.optim import lr_scheduler\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.validation import get_validation_recalls\n",
    "from models import helper\n",
    "\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9a2e93-ae05-45d9-abef-034b8d5b455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPRModel(pl.LightningModule):\n",
    "    \"\"\"This is the main model for Visual Place Recognition\n",
    "    we use Pytorch Lightning for modularity purposes.\n",
    "\n",
    "    Args:\n",
    "        pl (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                #---- Backbone\n",
    "                backbone_arch='resnet50',\n",
    "                pretrained=True,\n",
    "                layers_to_freeze=1,\n",
    "                layers_to_crop=[],\n",
    "                \n",
    "                #---- Aggregator\n",
    "                agg_arch='ConvAP', #CosPlace, NetVLAD, GeM\n",
    "                agg_config={},\n",
    "                \n",
    "                #---- Train hyperparameters\n",
    "                lr=0.03, \n",
    "                optimizer='sgd',\n",
    "                weight_decay=1e-3,\n",
    "                momentum=0.9,\n",
    "                warmpup_steps=500,\n",
    "                milestones=[5, 10, 15],\n",
    "                lr_mult=0.3,\n",
    "                \n",
    "                #----- Loss\n",
    "                loss_name='MultiSimilarityLoss', \n",
    "                miner_name='MultiSimilarityMiner', \n",
    "                miner_margin=0.1,\n",
    "                faiss_gpu=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.encoder_arch = backbone_arch\n",
    "        self.pretrained = pretrained\n",
    "        self.layers_to_freeze = layers_to_freeze\n",
    "        self.layers_to_crop = layers_to_crop\n",
    "\n",
    "        self.agg_arch = agg_arch\n",
    "        self.agg_config = agg_config\n",
    "\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.warmpup_steps = warmpup_steps\n",
    "        self.milestones = milestones\n",
    "        self.lr_mult = lr_mult\n",
    "\n",
    "        self.loss_name = loss_name\n",
    "        self.miner_name = miner_name\n",
    "        self.miner_margin = miner_margin\n",
    "        \n",
    "        self.save_hyperparameters() # write hyperparams into a file\n",
    "        \n",
    "        self.loss_fn = utils.get_loss(loss_name)\n",
    "        self.miner = utils.get_miner(miner_name, miner_margin)\n",
    "        self.batch_acc = [] # we will keep track of the % of trivial pairs/triplets at the loss level \n",
    "\n",
    "        self.faiss_gpu = faiss_gpu\n",
    "        \n",
    "        # ----------------------------------\n",
    "        # get the backbone and the aggregator\n",
    "        self.backbone = helper.get_backbone(backbone_arch, pretrained, layers_to_freeze, layers_to_crop)\n",
    "        self.aggregator = helper.get_aggregator(agg_arch, agg_config)\n",
    "        \n",
    "    # the forward pass of the lightning model\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.aggregator(x)\n",
    "        return x\n",
    "    \n",
    "    # configure the optimizer \n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer.lower() == 'sgd':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), \n",
    "                                        lr=self.lr, \n",
    "                                        weight_decay=self.weight_decay, \n",
    "                                        momentum=self.momentum)\n",
    "        elif self.optimizer.lower() == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                        lr=self.lr, \n",
    "                                        weight_decay=self.weight_decay)\n",
    "        elif self.optimizer.lower() == 'adam':\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                        lr=self.lr, \n",
    "                                        weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f'Optimizer {self.optimizer} has not been added to \"configure_optimizers()\"')\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.lr_mult)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    # configure the optizer step, takes into account the warmup stage\n",
    "    def optimizer_step(self,  epoch, batch_idx,\n",
    "                        optimizer, optimizer_idx, optimizer_closure,\n",
    "                        on_tpu, using_native_amp, using_lbfgs):\n",
    "        # warm up lr\n",
    "        if self.trainer.global_step < self.warmpup_steps:\n",
    "            lr_scale = min(1., float(self.trainer.global_step + 1) / self.warmpup_steps)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_scale * self.lr\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        \n",
    "    #  The loss function call (this method will be called at each training iteration)\n",
    "    def loss_function(self, descriptors, labels):\n",
    "        # we mine the pairs/triplets if there is an online mining strategy\n",
    "        if self.miner is not None:\n",
    "            miner_outputs = self.miner(descriptors, labels)\n",
    "            loss = self.loss_fn(descriptors, labels, miner_outputs)\n",
    "            \n",
    "            # calculate the % of trivial pairs/triplets \n",
    "            # which do not contribute in the loss value\n",
    "            nb_samples = descriptors.shape[0]\n",
    "            nb_mined = len(set(miner_outputs[0].detach().cpu().numpy()))\n",
    "            batch_acc = 1.0 - (nb_mined/nb_samples)\n",
    "\n",
    "        else: # no online mining\n",
    "            loss = self.loss_fn(descriptors, labels)\n",
    "            batch_acc = 0.0\n",
    "            if type(loss) == tuple: \n",
    "                # somes losses do the online mining inside (they don't need a miner objet), \n",
    "                # so they return the loss and the batch accuracy\n",
    "                # for example, if you are developping a new loss function, you might be better\n",
    "                # doing the online mining strategy inside the forward function of the loss class, \n",
    "                # and return a tuple containing the loss value and the batch_accuracy (the % of valid pairs or triplets)\n",
    "                loss, batch_acc = loss\n",
    "\n",
    "        # keep accuracy of every batch and later reset it at epoch start\n",
    "        self.batch_acc.append(batch_acc)\n",
    "        # log it\n",
    "        self.log('b_acc', sum(self.batch_acc) /\n",
    "                len(self.batch_acc), prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    # This is the training step that's executed at each iteration\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        places, labels = batch\n",
    "        \n",
    "        # Note that GSVCities yields places (each containing N images)\n",
    "        # which means the dataloader will return a batch containing BS places\n",
    "        BS, N, ch, h, w = places.shape\n",
    "        \n",
    "        # reshape places and labels\n",
    "        images = places.view(BS*N, ch, h, w)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Feed forward the batch to the model\n",
    "        descriptors = self(images) # Here we are calling the method forward that we defined above\n",
    "        loss = self.loss_function(descriptors, labels) # Call the loss_function we defined above\n",
    "        \n",
    "        self.log('loss', loss.item(), logger=True)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    # This is called at the end of eatch training epoch\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        # we empty the batch_acc list for next epoch\n",
    "        self.batch_acc = []\n",
    "\n",
    "    # For validation, we will also iterate step by step over the validation set\n",
    "    # this is the way Pytorch Lghtning is made. All about modularity, folks.\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        places, _ = batch\n",
    "        # calculate descriptors\n",
    "        descriptors = self(places)\n",
    "        return descriptors.detach().cpu()\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        \"\"\"this return descriptors in their order\n",
    "        depending on how the validation dataset is implemented \n",
    "        for this project (MSLS val, Pittburg val), it is always references then queries\n",
    "        [R1, R2, ..., Rn, Q1, Q2, ...]\n",
    "        \"\"\"\n",
    "        dm = self.trainer.datamodule\n",
    "        # The following line is a hack: if we have only one validation set, then\n",
    "        # we need to put the outputs in a list (Pytorch Lightning does not do it presently)\n",
    "        if len(dm.val_datasets)==1: # we need to put the outputs in a list\n",
    "            val_step_outputs = [val_step_outputs]\n",
    "        \n",
    "        for i, (val_set_name, val_dataset) in enumerate(zip(dm.val_set_names, dm.val_datasets)):\n",
    "            feats = torch.concat(val_step_outputs[i], dim=0)\n",
    "            \n",
    "            if 'pitts' in val_set_name:\n",
    "                # split to ref and queries\n",
    "                num_references = val_dataset.dbStruct.numDb\n",
    "                num_queries = len(val_dataset)-num_references\n",
    "                positives = val_dataset.getPositives()\n",
    "            elif 'msls' in val_set_name:\n",
    "                # split to ref and queries\n",
    "                num_references = val_dataset.num_references\n",
    "                num_queries = len(val_dataset)-num_references\n",
    "                positives = val_dataset.pIdx\n",
    "            else:\n",
    "                print(f'Please implement validation_epoch_end for {val_set_name}')\n",
    "                raise NotImplemented\n",
    "\n",
    "            r_list = feats[ : num_references]\n",
    "            q_list = feats[num_references : ]\n",
    "            pitts_dict = utils.get_validation_recalls(r_list=r_list, \n",
    "                                                q_list=q_list,\n",
    "                                                k_values=[1, 5, 10, 15, 20, 50, 100],\n",
    "                                                gt=positives,\n",
    "                                                print_results=True,\n",
    "                                                dataset_name=val_set_name,\n",
    "                                                faiss_gpu=self.faiss_gpu\n",
    "                                                )\n",
    "            del r_list, q_list, feats, num_references, positives\n",
    "\n",
    "            self.log(f'{val_set_name}/R1', pitts_dict[1], prog_bar=False, logger=True)\n",
    "            self.log(f'{val_set_name}/R5', pitts_dict[5], prog_bar=False, logger=True)\n",
    "            self.log(f'{val_set_name}/R10', pitts_dict[10], prog_bar=False, logger=True)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e33671-cdd6-4a47-bb56-1628800931c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN=[0.485, 0.456, 0.406]; STD=[0.229, 0.224, 0.225]\n",
    "\n",
    "IM_SIZE = (320, 320)\n",
    "\n",
    "def input_transform(image_size=IM_SIZE):\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "    if image_size:\n",
    "        return T.Compose(\n",
    "            [\n",
    "                T.Resize(image_size, interpolation=T.InterpolationMode.BILINEAR),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=MEAN, std=STD),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return T.Compose([T.ToTensor(), T.Normalize(mean=MEAN, std=STD)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e136c99-2116-449a-9368-51a96e7f6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.val.EssexDataset import EssexDataset\n",
    "\n",
    "def get_val_dataset(dataset_name, input_transform=input_transform()):\n",
    "    dataset_name = dataset_name.lower()\n",
    "    \n",
    "    if 'cross' in dataset_name:\n",
    "        ds = CrossSeasonDataset(input_transform = input_transform)\n",
    "    \n",
    "    elif 'essex' in dataset_name:\n",
    "        ds = EssexDataset(input_transform = input_transform)\n",
    "    \n",
    "    elif 'inria' in dataset_name:    \n",
    "        ds = InriaDataset(input_transform = input_transform)\n",
    "    \n",
    "    elif 'nordland' in dataset_name:    \n",
    "        ds = NordlandDataset(input_transform = input_transform)\n",
    "    \n",
    "    elif 'sped' in dataset_name:\n",
    "        ds = SPEDDataset(input_transform = input_transform)\n",
    "    \n",
    "    elif 'msls' in dataset_name:\n",
    "        ds = MSLS(input_transform = input_transform)\n",
    "\n",
    "    elif 'pitts' in dataset_name:\n",
    "        ds = PittsburghDataset(which_ds=dataset_name, input_transform = input_transform)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    num_references = ds.num_references\n",
    "    num_queries = ds.num_queries\n",
    "    ground_truth = ds.ground_truth\n",
    "    return ds, num_references, num_queries, ground_truth\n",
    "\n",
    "def get_descriptors(model, dataloader, device):\n",
    "    descriptors = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, 'Calculating descritptors...'):\n",
    "            imgs, labels = batch\n",
    "            output=model(imgs.to(device)).detach().cpu()\n",
    "            descriptors.append(output)\n",
    "\n",
    "    return torch.cat(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31d983d-3e42-43d1-934f-035bde8b34c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VPRModel(\n",
       "  (loss_fn): MultiSimilarityLoss(\n",
       "    (distance): DotProductSimilarity()\n",
       "    (reducer): MeanReducer()\n",
       "  )\n",
       "  (miner): MultiSimilarityMiner(\n",
       "    (distance): CosineSimilarity()\n",
       "  )\n",
       "  (backbone): ResNet(\n",
       "    (model): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): None\n",
       "      (avgpool): None\n",
       "      (fc): None\n",
       "    )\n",
       "  )\n",
       "  (aggregator): MixVPR(\n",
       "    (mix): Sequential(\n",
       "      (0): FeatureMixerLayer(\n",
       "        (mix): Sequential(\n",
       "          (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): FeatureMixerLayer(\n",
       "        (mix): Sequential(\n",
       "          (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): FeatureMixerLayer(\n",
       "        (mix): Sequential(\n",
       "          (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): FeatureMixerLayer(\n",
       "        (mix): Sequential(\n",
       "          (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (channel_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (row_proj): Linear(in_features=400, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#MixVPR\n",
    "model = VPRModel(\n",
    "    backbone_arch='resnet50', \n",
    "    layers_to_crop=[4],\n",
    "    agg_arch='MixVPR',\n",
    "    agg_config={\n",
    "        'in_channels' : 1024,\n",
    "        'in_h' : 20,\n",
    "        'in_w' : 20,\n",
    "        'out_channels' : 1024,\n",
    "        'mix_depth' : 4,\n",
    "        'mlp_ratio' : 1,\n",
    "        'out_rows' : 4\n",
    "    },\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19319cc1-87be-4cf9-a456-a590a4dd2c56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VPRModel(\n",
       "  (loss_fn): MultiSimilarityLoss(\n",
       "    (distance): DotProductSimilarity()\n",
       "    (reducer): MeanReducer()\n",
       "  )\n",
       "  (miner): MultiSimilarityMiner(\n",
       "    (distance): CosineSimilarity()\n",
       "  )\n",
       "  (backbone): ResNet(\n",
       "    (model): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): None\n",
       "      (avgpool): None\n",
       "      (fc): None\n",
       "    )\n",
       "  )\n",
       "  (aggregator): MixVPR(\n",
       "    (mix): Sequential(\n",
       "      (0): FeatureMixerLayer(\n",
       "        (mix): Sequential(\n",
       "          (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): FeatureMixerLayer(\n",
       "        (mix): Sequential(\n",
       "          (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): FeatureMixerLayer(\n",
       "        (mix): Sequential(\n",
       "          (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): FeatureMixerLayer(\n",
       "        (mix): Sequential(\n",
       "          (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (channel_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (row_proj): Linear(in_features=400, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('./saved_models/mixvpr/resnet50_MixVPR_4096_channels(1024)_rows(4).ckpt') # link to the trained weights\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71936d1-42db-4ac5-b3d5-22be0081ff9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9326c1860b7045ee8607affde074ca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating descritptors...:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor dimension 4096\n",
      "\n",
      "\n",
      "+------------------------------------+\n",
      "|        Performance on essex        |\n",
      "+----------+-------+--------+--------+\n",
      "|    K     |   1   |   5    |   10   |\n",
      "+----------+-------+--------+--------+\n",
      "| Recall@K | 88.10 | 100.00 | 100.00 |\n",
      "+----------+-------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "val_dataset_name = 'essex'\n",
    "batch_size = 10\n",
    "\n",
    "val_dataset, num_references, num_queries, ground_truth = get_val_dataset(val_dataset_name)\n",
    "val_loader = DataLoader(val_dataset, num_workers=4, batch_size=batch_size)\n",
    "\n",
    "descriptors = get_descriptors(model, val_loader, device)\n",
    "print(f'Descriptor dimension {descriptors.shape[1]}')\n",
    "\n",
    "# # now we split into references and queries\n",
    "r_list = descriptors[ : num_references].cpu()\n",
    "q_list = descriptors[num_references : ].cpu()\n",
    "recalls_dict, preds = get_validation_recalls(\n",
    "    r_list=r_list,\n",
    "    q_list=q_list,\n",
    "    k_values=[1, 5, 10],\n",
    "    gt=ground_truth,\n",
    "    print_results=True,\n",
    "    dataset_name=val_dataset_name,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12 (vpr-env)",
   "language": "python",
   "name": "vpr-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
